---
phase: 01-foundation-data-ingestion
plan: 02
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - job_monitor/backend/routers/jobs.py
  - job_monitor/backend/routers/billing.py
  - job_monitor/backend/models.py
  - job_monitor/backend/app.py
autonomous: true
requirements:
  - APP-05

must_haves:
  truths:
    - "API returns job run data from system.lakeflow.job_run_timeline"
    - "API returns job metadata from system.lakeflow.jobs with SCD2 handling"
    - "API returns billing data from system.billing.usage with RETRACTION handling"
    - "Queries use parameterized days filter for time range"
  artifacts:
    - path: "job_monitor/backend/routers/jobs.py"
      provides: "Job run and job metadata endpoints"
      contains: "system.lakeflow"
    - path: "job_monitor/backend/routers/billing.py"
      provides: "Billing usage endpoints"
      contains: "system.billing.usage"
    - path: "job_monitor/backend/models.py"
      provides: "Pydantic models for job runs, jobs, billing"
      contains: "JobRunListOut"
  key_links:
    - from: "job_monitor/backend/routers/jobs.py"
      to: "job_monitor/backend/core.py"
      via: "Depends(get_ws)"
      pattern: "Depends.*get_ws"
    - from: "job_monitor/backend/routers/jobs.py"
      to: "system.lakeflow.job_run_timeline"
      via: "SQL query execution"
      pattern: "system\\.lakeflow\\.job_run_timeline"
    - from: "job_monitor/backend/routers/billing.py"
      to: "system.billing.usage"
      via: "SQL query with RETRACTION handling"
      pattern: "HAVING SUM.*!= 0"
---

<objective>
Implement Unity Catalog system table ingestion for job runs, job metadata, and billing data.

Purpose: Enable the monitoring framework to query historical job execution data and cost information from system tables. This is the primary data source for all monitoring capabilities.

Output: API endpoints that return job runs, job metadata (with SCD2 handling), and billing data (with RETRACTION handling) from Unity Catalog system tables.
</objective>

<execution_context>
@/Users/laurent.prat/.claude/get-shit-done/workflows/execute-plan.md
@/Users/laurent.prat/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-data-ingestion/01-RESEARCH.md
@.planning/phases/01-foundation-data-ingestion/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement job run and job metadata endpoints</name>
  <files>
    job_monitor/backend/routers/jobs.py
    job_monitor/backend/models.py
    job_monitor/backend/app.py
  </files>
  <action>
1. Update `job_monitor/backend/models.py` with job models:
   ```python
   from pydantic import BaseModel
   from datetime import datetime
   from typing import Optional
   from enum import Enum

   class JobStatus(str, Enum):
       SUCCESS = "SUCCESS"
       FAILED = "FAILED"
       RUNNING = "RUNNING"
       PENDING = "PENDING"
       CANCELED = "CANCELED"
       SKIPPED = "SKIPPED"

   class JobRunListOut(BaseModel):
       run_id: str
       job_id: str
       period_start_time: datetime
       period_end_time: Optional[datetime] = None
       run_duration_seconds: Optional[int] = None
       result_state: Optional[str] = None

   class JobOut(BaseModel):
       job_id: str
       name: str
       creator_user_name: Optional[str] = None
       run_as_user_name: Optional[str] = None
       schedule: Optional[str] = None
   ```

2. Create `job_monitor/backend/routers/jobs.py`:
   - GET /api/jobs/runs: Query system.lakeflow.job_run_timeline
     - Parameter: days (int, default 7)
     - Returns list of JobRunListOut
     - Order by period_start_time DESC
     - Limit 1000 results

   - GET /api/jobs: Query system.lakeflow.jobs with SCD2 pattern
     - Returns list of JobOut (latest version of each job)
     - CRITICAL: Use ROW_NUMBER OVER PARTITION pattern for SCD2:
       ```sql
       WITH latest_jobs AS (
           SELECT *,
               ROW_NUMBER() OVER(
                   PARTITION BY workspace_id, job_id
                   ORDER BY change_time DESC
               ) as rn
           FROM system.lakeflow.jobs
           WHERE delete_time IS NULL
       )
       SELECT job_id, name, creator_user_name, run_as_user_name
       FROM latest_jobs
       WHERE rn = 1
       ```

3. Use asyncio.to_thread for SDK calls (Databricks SDK is synchronous):
   ```python
   import asyncio
   import os
   from fastapi import APIRouter, Depends
   from databricks.sdk import WorkspaceClient
   from ..core import get_ws

   api = APIRouter(prefix="/api")

   @api.get("/jobs/runs", response_model=list[JobRunListOut])
   async def list_job_runs(
       days: int = 7,
       ws: WorkspaceClient = Depends(get_ws)
   ):
       query = f"""
       SELECT run_id, job_id, period_start_time, period_end_time,
              run_duration_seconds, result_state
       FROM system.lakeflow.job_run_timeline
       WHERE period_start_time >= current_date() - INTERVAL {days} DAYS
       ORDER BY period_start_time DESC
       LIMIT 1000
       """
       result = await asyncio.to_thread(
           ws.statement_execution.execute_statement,
           warehouse_id=os.environ.get("WAREHOUSE_ID"),
           statement=query,
           wait_timeout="30s"
       )
       # Parse result.result.data_array into JobRunListOut models
       ...
   ```

4. Update `job_monitor/backend/app.py` to include jobs router.

NOTE: For local development without a warehouse, these endpoints will return mock data or an error. Full testing requires deployment.
  </action>
  <verify>
    Run: `cd /Users/laurent.prat/Documents/lpdev/databricks_job_monitoring && python -c "from job_monitor.backend.routers.jobs import api; print([r.path for r in api.routes])"`
    Expected output includes: /jobs/runs, /jobs
  </verify>
  <done>
    - JobRunListOut and JobOut models defined
    - /api/jobs/runs endpoint queries job_run_timeline with days parameter
    - /api/jobs endpoint queries jobs table with SCD2 pattern (ROW_NUMBER)
    - Jobs router included in main app
    - Async pattern used for SDK calls
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement billing data endpoint with RETRACTION handling</name>
  <files>
    job_monitor/backend/routers/billing.py
    job_monitor/backend/models.py
    job_monitor/backend/app.py
  </files>
  <action>
1. Add billing models to `job_monitor/backend/models.py`:
   ```python
   class BillingUsageOut(BaseModel):
       usage_date: str  # DATE type from SQL
       job_id: Optional[str] = None
       cluster_id: Optional[str] = None
       sku_name: str
       total_dbus: float
   ```

2. Create `job_monitor/backend/routers/billing.py`:
   - GET /api/billing/usage: Query system.billing.usage
     - Parameter: days (int, default 30)
     - Returns list of BillingUsageOut
     - CRITICAL: Handle RETRACTION records with HAVING clause:
       ```sql
       SELECT
           usage_date,
           usage_metadata.job_id as job_id,
           usage_metadata.cluster_id as cluster_id,
           sku_name,
           SUM(usage_quantity) AS total_dbus
       FROM system.billing.usage
       WHERE usage_date >= current_date() - INTERVAL {days} DAYS
       GROUP BY usage_date, usage_metadata.job_id, usage_metadata.cluster_id, sku_name
       HAVING SUM(usage_quantity) != 0
       ORDER BY usage_date DESC, total_dbus DESC
       LIMIT 1000
       ```

   - GET /api/billing/by-job: Aggregate billing by job
     - Parameter: days (int, default 30)
     - Groups by job_id
     - Returns total DBUs per job over period

3. Important considerations from research:
   - usage_metadata.job_id is NULL for all-purpose compute (document this limitation)
   - RETRACTION records have negative quantities - the HAVING clause filters fully retracted items
   - SKU names vary (ALL_PURPOSE_COMPUTE, JOBS_COMPUTE, SERVERLESS_SQL, etc.)

4. Update `job_monitor/backend/app.py` to include billing router.
  </action>
  <verify>
    Run: `cd /Users/laurent.prat/Documents/lpdev/databricks_job_monitoring && python -c "from job_monitor.backend.routers.billing import api; print([r.path for r in api.routes])"`
    Expected output includes: /billing/usage, /billing/by-job
  </verify>
  <done>
    - BillingUsageOut model defined
    - /api/billing/usage endpoint queries system.billing.usage
    - HAVING SUM != 0 pattern used for RETRACTION handling
    - /api/billing/by-job aggregates by job_id
    - Billing router included in main app
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. All routers import without errors
2. App starts: `uvicorn job_monitor.backend.app:app --reload`
3. OpenAPI docs at /docs show all endpoints:
   - /api/jobs/runs
   - /api/jobs
   - /api/billing/usage
   - /api/billing/by-job
4. SQL queries in code follow documented patterns:
   - SCD2: ROW_NUMBER OVER PARTITION BY
   - RETRACTION: HAVING SUM != 0
</verification>

<success_criteria>
- 4 new API endpoints defined and documented in OpenAPI
- SCD2 pattern correctly implemented for jobs table
- RETRACTION handling correctly implemented for billing
- All endpoints use async pattern with asyncio.to_thread
- Models match expected system table schemas
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-ingestion/01-02-SUMMARY.md`
</output>
