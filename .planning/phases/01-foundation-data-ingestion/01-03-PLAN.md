---
phase: 01-foundation-data-ingestion
plan: 03
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - job_monitor/backend/routers/jobs_api.py
  - job_monitor/backend/models.py
  - job_monitor/backend/app.py
autonomous: false
requirements:
  - APP-06

user_setup:
  - service: databricks
    why: "SQL Warehouse required for system table queries"
    env_vars:
      - name: WAREHOUSE_ID
        source: "Databricks Workspace -> SQL Warehouses -> select warehouse -> copy ID from URL"
      - name: DATABRICKS_HOST
        source: "Databricks Workspace URL (e.g., https://xxx.cloud.databricks.com)"

must_haves:
  truths:
    - "API returns real-time job status from Jobs API (not system tables)"
    - "API returns job task details including parameters"
    - "API returns active/running jobs for monitoring"
    - "App deploys to Databricks and is accessible via workspace URL"
  artifacts:
    - path: "job_monitor/backend/routers/jobs_api.py"
      provides: "Jobs API endpoints for real-time data"
      contains: "ws.jobs.list"
    - path: "job_monitor/backend/models.py"
      provides: "Models for Jobs API responses"
      contains: "JobApiRunOut"
  key_links:
    - from: "job_monitor/backend/routers/jobs_api.py"
      to: "databricks.sdk.WorkspaceClient.jobs"
      via: "SDK method calls"
      pattern: "ws\\.jobs\\.(list|list_runs|get)"
    - from: "job_monitor/backend/app.py"
      to: "all routers"
      via: "include_router"
      pattern: "include_router.*jobs_api"
---

<objective>
Implement Jobs API integration for real-time data not available in system tables, and verify deployment.

Purpose: System tables have 5-15 minute latency. The Jobs API provides real-time status for active jobs, task-level details, and repair history that aren't available in system tables. This completes the data ingestion layer.

Output: API endpoints using Jobs API for real-time data, successfully deployed Databricks App accessible via workspace URL.
</objective>

<execution_context>
@/Users/laurent.prat/.claude/get-shit-done/workflows/execute-plan.md
@/Users/laurent.prat/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-data-ingestion/01-RESEARCH.md
@.planning/phases/01-foundation-data-ingestion/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Jobs API endpoints for real-time data</name>
  <files>
    job_monitor/backend/routers/jobs_api.py
    job_monitor/backend/models.py
    job_monitor/backend/app.py
  </files>
  <action>
1. Add Jobs API models to `job_monitor/backend/models.py`:
   ```python
   class JobApiOut(BaseModel):
       job_id: int
       name: str
       creator_user_name: Optional[str] = None
       created_time: Optional[datetime] = None
       settings_format: Optional[str] = None  # SINGLE_TASK, MULTI_TASK

   class JobApiRunOut(BaseModel):
       run_id: int
       job_id: int
       run_name: Optional[str] = None
       state: str  # PENDING, RUNNING, TERMINATED, etc.
       result_state: Optional[str] = None  # SUCCESS, FAILED, etc.
       start_time: Optional[datetime] = None
       end_time: Optional[datetime] = None
       run_page_url: Optional[str] = None

   class ActiveRunsOut(BaseModel):
       total_active: int
       runs: list[JobApiRunOut]
   ```

2. Create `job_monitor/backend/routers/jobs_api.py`:
   - GET /api/jobs-api/list: List all jobs via Jobs API
     - Uses ws.jobs.list()
     - Returns list of JobApiOut
     - Useful for: discovering jobs not in system tables (365-day retention limit)

   - GET /api/jobs-api/runs/{job_id}: Get recent runs for a specific job
     - Uses ws.jobs.list_runs(job_id=job_id, limit=20)
     - Returns list of JobApiRunOut
     - Useful for: real-time status of critical jobs

   - GET /api/jobs-api/active: Get all currently active runs
     - Uses ws.jobs.list_runs(active_only=True)
     - Returns ActiveRunsOut with count and run list
     - Useful for: monitoring dashboard "currently running" widget

3. Implementation pattern (async wrapping required):
   ```python
   import asyncio
   from fastapi import APIRouter, Depends
   from databricks.sdk import WorkspaceClient
   from ..core import get_ws
   from ..models import JobApiOut, JobApiRunOut, ActiveRunsOut

   api = APIRouter(prefix="/api/jobs-api", tags=["Jobs API"])

   @api.get("/list", response_model=list[JobApiOut])
   async def list_jobs_api(
       limit: int = 100,
       ws: WorkspaceClient = Depends(get_ws)
   ):
       jobs = await asyncio.to_thread(lambda: list(ws.jobs.list(limit=limit)))
       return [
           JobApiOut(
               job_id=j.job_id,
               name=j.settings.name if j.settings else "Unknown",
               creator_user_name=j.creator_user_name,
               created_time=datetime.fromtimestamp(j.created_time / 1000) if j.created_time else None,
               settings_format=j.settings.format.value if j.settings and j.settings.format else None
           )
           for j in jobs
       ]

   @api.get("/active", response_model=ActiveRunsOut)
   async def get_active_runs(ws: WorkspaceClient = Depends(get_ws)):
       runs = await asyncio.to_thread(lambda: list(ws.jobs.list_runs(active_only=True)))
       run_models = [
           JobApiRunOut(
               run_id=r.run_id,
               job_id=r.job_id,
               run_name=r.run_name,
               state=r.state.life_cycle_state.value if r.state else "UNKNOWN",
               result_state=r.state.result_state.value if r.state and r.state.result_state else None,
               start_time=datetime.fromtimestamp(r.start_time / 1000) if r.start_time else None,
               run_page_url=r.run_page_url
           )
           for r in runs
       ]
       return ActiveRunsOut(total_active=len(run_models), runs=run_models)
   ```

4. Update `job_monitor/backend/app.py` to include jobs_api router.

NOTE: These endpoints require valid Databricks credentials. Local testing requires:
- DATABRICKS_HOST environment variable
- Valid authentication (token or default profile)
  </action>
  <verify>
    Run: `cd /Users/laurent.prat/Documents/lpdev/databricks_job_monitoring && python -c "from job_monitor.backend.routers.jobs_api import api; print([r.path for r in api.routes])"`
    Expected output includes: /list, /runs/{job_id}, /active
  </verify>
  <done>
    - JobApiOut, JobApiRunOut, ActiveRunsOut models defined
    - /api/jobs-api/list endpoint calls ws.jobs.list()
    - /api/jobs-api/runs/{job_id} endpoint calls ws.jobs.list_runs()
    - /api/jobs-api/active endpoint returns currently running jobs
    - All endpoints use async pattern with asyncio.to_thread
    - jobs_api router included in main app
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify local development and deployment readiness</name>
  <what-built>
    Complete Phase 1 implementation:
    - APX project structure with FastAPI backend
    - OAuth authentication with user identity extraction
    - System table endpoints (jobs, job_runs, billing)
    - Jobs API endpoints for real-time data
    - Databricks Asset Bundle configuration
  </what-built>
  <how-to-verify>
    1. Start the local server:
       ```bash
       cd /Users/laurent.prat/Documents/lpdev/databricks_job_monitoring
       uvicorn job_monitor.backend.app:app --reload
       ```

    2. Open http://localhost:8000/docs in browser
       - Verify all endpoints are documented:
         - GET /api/health
         - GET /api/me
         - GET /api/jobs/runs
         - GET /api/jobs
         - GET /api/billing/usage
         - GET /api/billing/by-job
         - GET /api/jobs-api/list
         - GET /api/jobs-api/runs/{job_id}
         - GET /api/jobs-api/active

    3. Test health endpoint:
       ```bash
       curl http://localhost:8000/api/health
       ```
       Expected: {"status": "ok", "version": "0.1.0"}

    4. Test user endpoint:
       ```bash
       curl http://localhost:8000/api/me
       ```
       Expected: {"email": "local-dev-user", ...}

    5. Verify configuration files exist:
       - databricks.yml (bundle config)
       - app.yaml (app config with OAuth scopes)
       - pyproject.toml (dependencies)

    6. (Optional) If you have Databricks credentials configured:
       ```bash
       export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
       curl http://localhost:8000/api/jobs-api/list
       ```
       Should return list of jobs from your workspace
  </how-to-verify>
  <resume-signal>Type "approved" if all endpoints work, or describe any issues encountered</resume-signal>
</task>

</tasks>

<verification>
After task completion:
1. All 9 API endpoints accessible at /docs
2. Local server starts without errors
3. Health and user endpoints return expected responses
4. Configuration files ready for deployment
5. Code follows async patterns from research
</verification>

<success_criteria>
- Jobs API endpoints provide real-time data supplementing system tables
- All endpoints documented in OpenAPI spec
- Local development server runs successfully
- Project structure ready for `databricks bundle deploy`
- User confirms endpoints work locally
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-data-ingestion/01-03-SUMMARY.md`
</output>
